########METHOD1 CODE##########
Step 1: 2 related securities ( two securities those have similiraties: same sector / industry/ similar market capitalization/average volume traded.)
Step 2: spread ( pair ratio to indicate the spread==price of asset A / price asset B)
Step 3: mean, standard deviation, and z-score of the pair ratio / spread.
Step 4: Test for co-integration(Augmented Dicky Fuller Test (ADF Test) to test for co-integration. all tests have to reject the null hypothesis that the pair is not co-integrated)
Step 5: Generate trading signals
Step 6: Process transactions based on signals
Step 7: Reporting
*Charting:
An Equity curve
Drawdown curve
Daily returns bar chart
Statistics:
*Annual Returns
Annualized Sharpe Ratio
Maximum Drawdown
Total trades
Success ratio
PnL ratio
*Table:
Top 5 Drawdowns and their duration


######FUNCTIONS
#Add Columns to csvDataframe
AddColumns <- function(csvData){
csvData$spread <- 0
csvData$adfTest <- 0
csvData$mean <- 0
csvData$stdev <- 0
csvData$zScore <- 0
csvData$signal <- 0
csvData$BuyPrice <- 0
csvData$SellPrice <- 0
csvData$LongReturn <- 0
csvData$ShortReturn <- 0
csvData$Slippage <- 0
csvData$TotalReturn <- 0
return(csvData)
}

#PrepareData function calculates the pair ratio and the log10 prices of the pair.
PrepareData <- function(csvData){
#Calculate the Pair Ratio
csvData$pairRatio <- csvData[,2] / csvData[,3]
#Calculate the log prices of the two time series
csvData$LogA <- log10(csvData[,2])
csvData$LogB <- log10(csvData[,3])
#Add columns to the DF
csvData <- AddColumns(csvData)
#Make sure that the date column is not read in as a vector of characters
csvData$Date <- as.Date(csvData$Date)
return(csvData)
}

#Calculate mean, stdDev, and z-score for the given Row [end]
GenerateRowValue <- function(begin, end, csvData){
average <- mean(csvData$spread[begin:end])
stdev <- sd(csvData$spread[begin:end])
csvData$mean[end] <- average
csvData$stdev[end] <- stdev
csvData$zScore[end] <- (csvData$spread[end]-average)/stdev
return(csvData) 
}

#Generate trading signals based on a z-score of 1 and -1 
GenerateSignal <- function(counter, csvData){
#Trigger and close represent the entry and exit zones (value refers to the z-score value)
trigger <- 1
close <- 0.5
currentSignal <- csvData$signal[counter]
prevSignal <- csvData$signal[counter-1]
#Set trading signal for the given [counter] row
if(csvData$adfTest[counter] == 1)
{
   #If there is a change in signal from long to short then you must allow for the current trade to first be closed
   if(currentSignal == -1 && prevSignal == 1)
     csvData$signal[counter] <- 0
   else if(currentSignal == 1 && prevSignal == -1)
     csvData$signal[counter] <- 0
   
   #Create a long / short signal if the current z-score is larger / smaller than the trigger value (respectively)
   else if(csvData$zScore[counter] > trigger)
     csvData$signal[counter] <- -1
   else if (csvData$zScore[counter] < -trigger)
     csvData$signal[counter] <- 1
   
   #Close the position if z-score is beteween the two "close" values
   else if (csvData$zScore[counter] < close && csvData$zScore[counter] > -close)
     csvData$signal[counter] <- 0
   else 
     csvData$signal[counter] <- prevSignal
}
else 
   csvData$signal[counter] <- 0
return(csvData)
}

####Transactions based on trade signal
GenerateTransactions <- function(currentSignal, prevSignal, end, csvData){
#In a pair trading strategy you need to go long one share and short the other and then reverse the transaction when you close
##First Leg of the trade (Set Long position)
#If there is no change in signal
if(currentSignal == 0 && prevSignal == 0)
   csvData$BuyPrice[end] <- 0   
else if(currentSignal == prevSignal)
   csvData$BuyPrice[end] <- csvData$BuyPrice[end-1]     
#If the signals point to a new trade
#Short B and Long A
else if(currentSignal == 1 && currentSignal != prevSignal)
   csvData$BuyPrice[end] <- csvData[end, 2] 
#Short A and Long B
else if(currentSignal == -1 && currentSignal != prevSignal){
   csvData$BuyPrice[end] <- csvData[end, 3] * csvData$pairRatio[end]
   transactionPairRatio <<- csvData$pairRatio[end]
}
#Close trade
else if(currentSignal == 0 && prevSignal == 1)
   csvData$BuyPrice[end] <- csvData[end, 2] 
else if(currentSignal == 0 && prevSignal == -1)
   csvData$BuyPrice[end] <- csvData[end, 3] * transactionPairRatio 
##Second Leg of the trade (Set Short position)
##Set Short Prices if there is no change in signal
if(currentSignal == 0 && prevSignal == 0)
   csvData$SellPrice[end] <- 0   
else if(currentSignal == prevSignal)
   csvData$SellPrice[end] <- csvData$SellPrice[end-1] 
#If the signals point to a new trade
else if(currentSignal == 1 && currentSignal != prevSignal){
   csvData$SellPrice[end] <- csvData[end, 3] * csvData$pairRatio[end]
   transactionPairRatio <<- csvData$pairRatio[end]
}
else if(currentSignal == -1 && currentSignal != prevSignal)
   csvData$SellPrice[end] <- csvData[end, 2] 
#Close trade
else if(currentSignal == 0 && prevSignal == 1){
   csvData$SellPrice[end] <- csvData[end, 3] * transactionPairRatio
}
else if(currentSignal == 0 && prevSignal == -1)
   csvData$SellPrice[end] <- csvData[end, 2] 
return(csvData)
}
####GetReturns calculates the returns on each position after it has been closed and then calculates the total returns and adds slippage.
#Calculate the returns generated after each transaction
#Add implementation shortfall / slippage
GetReturns <- function(end, csvData, slippage){
#Calculate the returns generated on each leg of the deal (the long and the short position)
#Long leg of the trade
if(csvData$signal[end] == 0 && csvData$signal[end-1] != 0 )
   csvData$LongReturn[end] <- (csvData$BuyPrice[end] / csvData$BuyPrice[end-1]) - 1
#Short Leg of the trade
if(csvData$signal[end] == 0 && csvData$signal[end-1] != 0 )
   csvData$ShortReturn[end] <- (csvData$SellPrice[end-1] / csvData$SellPrice[end]) - 1
#Add slippage
if(csvData$ShortReturn[end] != 0)
   csvData$Slippage[end] <- slippage
#If a trade was closed then calculate the total return
if(csvData$ShortReturn[end] != 0 && csvData$LongReturn[end] != 0)
   csvData$TotalReturn[end] <- ((csvData$ShortReturn[end] + csvData$LongReturn[end]) / 2) + csvData$Slippage[end]
return(csvData)
}

#Returns an equity curve, annualized return, annualized sharpe ratio, and max drawdown
GenerateReport <- function(pairData, startDate, endDate){
#Subset the dates 
returns <- xts(pairData$TotalReturn, as.Date(pairData$Date))
returns <- returns[paste(startDate,endDate,sep="::")]
#Plot
charts.PerformanceSummary(returns)
#Metrics
print(paste("Annual Returns: ",Return.annualized(returns)))
print(paste("Annualized Sharpe: " ,SharpeRatio.annualized(returns)))
print(paste("Max Drawdown: ",maxDrawdown(returns)))
#var returns = xts object
totalTrades <- 0
positiveTrades <- 0
profitsVector <- c()
lossesVector <- c()
#loop through the data to find the + & - trades and total trades
for(i in returns){
   if(i != 0){
     totalTrades <- totalTrades + 1
     if(i > 0){
       positiveTrades <- positiveTrades + 1
       profitsVector <- c(profitsVector, i)
     }
     else if (i < 0){
       lossesVector <- c(lossesVector, i)
     }
   }
}
#Print the results to the console
print(paste("Total Trades: ", totalTrades))
print(paste("Success Rate: ", positiveTrades/totalTrades))
print(paste("PnL Ratio: ", mean(profitsVector)/mean(lossesVector*-1)))
print(table.Drawdowns(returns))
}
#Use this one if you have the returns in xts format and want to generate a report
GenerateReport.xts <- function(returns, startDate = '2005-01-01', endDate = '2015-11-23'){
returns <- returns[paste(startDate,endDate,sep="::")]
#Plot
charts.PerformanceSummary(returns)
#Metrics
print(paste("Annual Returns: ",Return.annualized(returns)))
print(paste("Annualized Sharpe: " ,SharpeRatio.annualized(returns)))
print(paste("Max Drawdown: ",maxDrawdown(returns)))
#var returns = xts object
totalTrades <- 0
positiveTrades <- 0
profitsVector <- c()
lossesVector <- c()
#Itterate through data to get the + & - trades
for(i in returns){
   if(i != 0){
     totalTrades <- totalTrades + 1
     if(i > 0){
       positiveTrades <- positiveTrades + 1
       profitsVector <- c(profitsVector, i)
     }
     else if (i < 0){
       lossesVector <- c(lossesVector, i)
     }
   }
}
#Print results to Console
print(paste("Total Trades: ", totalTrades))
print(paste("Success Rate: ", positiveTrades/totalTrades))
print(paste("PnL Ratio: ", mean(profitsVector)/mean(lossesVector*-1)))
print(table.Drawdowns(returns))
}

####BacktestPair
#Function’s arguments:
#pairData = the csv file date
#mean = the number of observations used to calculate the mean of the spread.
#slippage = the amount of basis points that act as brokerage as well as slippage
#adfTest = a boolean value – if the backtest should test for co-integration
#criticalValue = Critical Value used in the ADF Test to test for co-integration
#generateReport = a boolean value – if a report must be generated
#The function that will be called by the user to backtest a pair
BacktestPair <- function(pairData, mean = 35, slippage = -0.0020, adfTest = TRUE, criticalValue = -2.58, startDate = '2005-01-01', endDate = '2014-11-23', generateReport = TRUE){
# At 150 data points
# Critical value at 1% : -3.46
# Critical value at 5% : -2.88
# Critical value at 10% : -2.57
#Prepare the initial dataframe by adding columns and pre calculations
pairData <- PrepareData(pairData)
#Itterate through each day in the time series
for(i in 1:length(pairData[,2])){
   
   #For each day after the amount of days needed to run the ADF test
   if(i > 130){
     begin <- i - mean + 1
     end <- i
     
     #Calculate Spread
    spread <- pairData$pairRatio[end]
     pairData$spread[end] <- spread
     
     #ADF Test for co-integration
     #120 - 90 - 60 
     if(adfTest == FALSE){
       pairData$adfTest[end] <- 1 
     }
     else {
       if(adf.test(pairData$spread[(i-120):end], k = 1)[1] <= criticalValue){
         if(adf.test(pairData$spread[(i-90):end], k = 1)[1] <= criticalValue){
           if(adf.test(pairData$spread[(i-60):end], k = 1)[1] <= criticalValue){
             #If co-integrated then set the ADFTest value to true / 1
             pairData$adfTest[end] <- 1           
           }
         }
       }
     }
     
     #Calculate the remainder variables needed
     if(i >= mean){
       #Generate Row values
       pairData <- GenerateRowValue(begin, end, pairData)
       
       #Generate the Signals
       pairData <- GenerateSignal(i, pairData)
       
       currentSignal <- pairData$signal[i]
       prevSignal <- pairData$signal[i-1]
       
       #Generate Transactions
       pairData <- GenerateTransactions(currentSignal, prevSignal, i, pairData)
       
       #Get the returns with added slippage
       pairData <- GetReturns(i, pairData, slippage)
     }
   }
}
if(generateReport == TRUE)
   GenerateReport(pairData, startDate, endDate)
return(pairData)
}

####BacktestPortfolio
#Function’s arguments:
#names = an atomic vector of csv file names, example: c(‘DsyLib.csv’, ‘OldSanlam.csv’)
#mean = the number of observations used to calculate the mean of the spread.
#leverage = how much leverage you want to apply to the portfolio
 #An equally weighted portfolio of shares
BacktestPortfolio <- function(names, mean = 35,leverage = 1, startDate = '2005-01-01', endDate = '2015-11-23'){
##Iterates through all the pairs and backtests each one
##stores the data in a list of numerical vectors
returns.list <- list()
counter <- F
ticker <- 1
for (name in names){
   #A notification to let you know how far it is (Uncomment this if you   want notifications
   #print(paste(ticker, " of ", length(names)))
   #ticker <- ticker + 1
   
   #Run the backtest on the pair
   data <- read.csv(name)   
   BackTest.df <- BacktestPair(data, mean, generateReport = FALSE)
   
   #Store the dates in a seperate vector
   if (counter == F){
     dates <<- as.Date(BackTest.df$Date)
     counter <- T
   }
   
   #Append to list
   returns.list <- c(returns.list, list(BackTest.df[,18]))
}
##Aggregates the returns for each day and then calculates the average for each day
total.returns <- c()
for (i in 1:length(returns.list)){
   if(i == 1)
     total.returns = returns.list[[i]]
   else
     total.returns = total.returns + returns.list[[i]]
}
total.returns <- total.returns / length(returns.list)
##Generate a report for the portfolio
returns <- xts(total.returns * leverage, dates)
GenerateReport.xts(returns, startDate, endDate)
return(returns)
}

#####TESTING
#Imports
require(tseries)
require(urca) #Used for the ADF Test
require(PerformanceAnalytics)
require(quantmod)
library(chron)
#load
getSymbols(c("SPY","^GSPC"))

#seq(as.Date("2007-01-03 "), length.out=2308, by="1 day")
#y0<- seq(as.Date("2007-01-03 "), as.Date("2016-03-03 "), by="1 day")
#y0<-y0[!is.weekend(y0)]
#y0<-y0[!is.holiday(y0)]
y0<-as.Date(row(Cl(SPY)),"2007-01-02")
y1<-ts(Cl(SPY))[,1]
y2<-ts(Cl(GSPC))[,1]
y3<-merge(y1,y2)
data<-na.omit(data.frame(y0,y1,y2))
colnames(data)<-c("Date","SPY","GSPC") #name our columns 

#In-Sample Test 
leverage <- 3
testpair <- BacktestPair(data, 35, generateReport = F, adfTest = F) 
#Format to an xts object and pass to GenerateReport.xts()
testpair.returns <- xts(testpair[,18] * leverage, testpair$Date)
GenerateReport.xts(testpair.returns, startDate = '2007-01-03', endDate = '2012-08-15')	
#out of Sample Test 
GenerateReport.xts(testpair.returns, startDate = '2012-08-16', endDate = '2013-11-23')

###Contruction Portfolio
#In-sample test
names <- c('groupmr.csv', 'groupppc.csv', 'groupavenge.csv', 'groupwhbo.csv', 'mrppc.csv', 'mravenge.csv')
ReturnSeries <- BacktestPortfolio(names, startDate = '2005-01-01', endDate = '2012-11-01', leverage = 3)
#Out-of-sample test 
GenerateReport.xts(ReturnSeries, startDate = '2012-11-23', endDate = '2015-11-23')

#s://github.com/Jackal08/QuantInsti-Final-Project-Statistical-Arbitrage/blob/master/R%20Code/StatisticalArbitrage.R
###END######

#############METHOD2 CODE####

If the price ratio of two stocks is a mean reverting process then a simple pairs trading strategy could be to assume the ratio has a gaussian distribution and construct a 90% confidence interval and go long (short) the numerator stock, stock A, and go short (long) the denominator stock, stock B, when the price ratio rises above (falls below) the respective threshold. However, in practice the mean of the price ratio can often exhibit regime switches where the ratio stays high or low for sustained periods. This can lead to the basic pairs trading strategy outlined above performing poorly.

########################################################
########################################################
######## MARKOV SWITCHING PAIR TRADING FUNCTIONS ####### 
########################################################
########################################################


# # # # # hamilton filter # # # # #

hamilton.filter <- function(theta, y, ss.prob, ps0=1/2, ps1=1/2) {

# construct parameters
 
 mu_s0 <- theta[1]
 mu_s1 <- theta[2]
 sigma2_s0 <- exp(theta[3])
 sigma2_s1 <- exp(theta[4])
 gamma_s0 <- theta[5]
 gamma_s1 <- theta[6]
 
# construct probabilities

#probit specification
  
p_s0_s0 <- pnorm(gamma_s0)
p_s0_s1 <- pnorm(gamma_s1)
p_s1_s0 <- 1-pnorm(gamma_s0)
p_s1_s1 <- 1-pnorm(gamma_s1)

# # # construct transition probability matrix P # # #

T.mat <- matrix(c(p_s0_s0, p_s1_s0, p_s0_s1, p_s1_s1), nrow = 2, ncol = 2, byrow = TRUE)

if(ss.prob == TRUE) {

# assume erogodicity of the markov chain
# use unconditional probabilities

p0_s0 <- (1 - p_s1_s1) / (2 -p_s0_s0 -p_s1_s1)
p0_s1 <- 1-p0_s0

}

else {
 
 p0_s0 <- ps0
 p0_s1 <- ps1
 
 }

# create variables

n <- length(y)

p_s0_t_1 <- rep(0,n)
p_s1_t_1 <- rep(0,n)
p_s0_t <- rep(0,n)
p_s1_t <- rep(0,n)
f_s0 <- rep(0,n-1)
f_s1 <- rep(0,n-1)
f <- rep(0,n-1)
logf <- rep(0,n-1)

p_s0_t[1] <- p0_s0
p_s1_t[1] <- p0_s1
p_s0_t_1[1] <- p0_s0 
p_s1_t_1[1] <- p0_s1
# initiate hamilton filter

for(i in 2:n) {
 
# calculate prior probabilities using the TPT
# TPT for this example gives us 
# p_si_t_1 = p_si_t_1_si * p_si_t + p_si_t_1_sj * p_si_t
# where p_si_t_1 is the prob state_t = i given information @ time t-1
# p_si_t_1_sj is the prob state_t = i given state_t_1 = j, and all info @ time t-1
# p_si_t is the prob state_t = i given information @ time t

# in this simple example p_si_t_1_sj = p_si_sj

p_s0_t_1[i] <- (p_s0_s0 * p_s0_t[i-1]) + (p_s0_s1 * p_s1_t[i-1])
p_s1_t_1[i] <- (p_s1_s0 * p_s0_t[i-1]) + (p_s1_s1 * p_s1_t[i-1])

# calculate density function for observation i
# f_si is density conditional on state = i
# f is the density 
  
f_s0[i] <- dnorm(y[i]-mu_s0, sd = sqrt(sigma2_s0)) 
f_s1[i] <- dnorm(y[i]-mu_s1, sd = sqrt(sigma2_s1))
f[i] <- (f_s0[i] * p_s0_t_1[i]) + (f_s1[i] * p_s1_t_1[i])

# calculate filtered/posterior probabilities using bayes rule
# p_si_t is the prob that state = i given information @ time t

p_s0_t[i] <- (f_s0[i] * p_s0_t_1[i]) / f[i]
p_s1_t[i] <- (f_s1[i] * p_s1_t_1[i]) / f[i]

logf[i] <- log(f[i])

}

logl <-sum(logf)

output <- list(p_s0_t_1 = p_s0_t_1, p_s1_t_1 = p_s1_t_1,f.prob_s0 = p_s0_t,f.prob_s1 = p_s1_t,T.mat = T.mat,logLik = logl)

return(output)

}

########################################################
########################################################

# # # # # Kim's smoothing algorithm # # # # #

kim.smooth <- function(T.mat,fp_s0,fp_s1,p_s0_t_1,p_s1_t_1) {

n <- length(fp_s0)

# create variables
 
 p_s0_T <- rep(0,n) 
 p_s1_T <- rep(0,n)
 p_s0_s0_t_1 <- rep(0,n-1)
 p_s0_s1_t_1 <- rep(0,n-1)
 p_s1_s0_t_1 <- rep(0,n-1)
 p_s1_s1_t_1 <- rep(0,n-1)
 
# assign initial values to implement kim's algo

    p_s0_T[n] <- fp_s0[n]
    p_s1_T[n] <- fp_s1[n]

# initiate recursion

for(i in (n-1):1) {
 
 p_s0_s0_t_1[i] <- fp_s0[i] * T.mat[1,1] * (p_s0_T[i+1]/p_s0_t_1[i+1])
 p_s0_s1_t_1[i] <- fp_s0[i] * T.mat[1,2] * (p_s1_T[i+1]/p_s1_t_1[i+1])
 p_s0_T[i] <- p_s0_s0_t_1[i] + p_s0_s1_t_1[i]
 
 p_s1_s0_t_1[i] <- fp_s1[i] * T.mat[2,1] * (p_s0_T[i+1]/p_s0_t_1[i+1])
 p_s1_s1_t_1[i] <- fp_s1[i] * T.mat[2,2] *(p_s1_T[i+1]/p_s1_t_1[i+1])
 p_s1_T[i] <- p_s1_s0_t_1[i] + p_s1_s1_t_1[i]
 
 }

output <- list(p_s0_s0_t_1 = p_s0_s0_t_1, p_s0_s1_t_1 = p_s0_s1_t_1, p_s1_s0_t_1 = p_s1_s0_t_1, p_s1_s1_t_1 = p_s1_s1_t_1, p_s0_T = p_s0_T, p_s1_T = p_s1_T)
return(output)

}
########################################################
########################################################

# # # # # EM algorithm # # # # #

EM <- function(y,tol,start_val) {

out <- hamilton.filter(start_val,y,ss.prob = TRUE)

fprob_s1 <- out$f.prob_s1
fprob_s0 <- out$f.prob_s0
p_s0_t_1 <- out$p_s0_t_1
p_s1_t_1 <- out$p_s1_t_1
T.mat <- out$T.mat

s.out <- kim.smooth(T.mat,fp_s0 = out$f.prob_s0 , fp_s1 = out$f.prob_s1, p_s0_t_1 = out$p_s0_t_1, p_s1_t_1 = out$p_s1_t_1)

p_s0_T <- s.out$p_s0_T
p_s1_T <- s.out$p_s1_T
p_s0_s0_t_1 <- s.out$p_s0_s0_t_1
p_s0_s1_t_1 <- s.out$p_s0_s1_t_1
p_s1_s0_t_1 <- s.out$p_s1_s0_t_1
p_s1_s1_t_1 <- s.out$p_s1_s1_t_1

# first iteration of the transition probabilities 

p00_k <- sum(p_s0_s0_t_1) / sum(p_s0_T[2:length(p_s0_T)])

p01_k <- sum(p_s0_s1_t_1) / sum(p_s1_T[2:length(p_s1_T)])
# recall we used a probit parameterization for the probabilities so we need to backout gamma_s0 and gamma_s1

gamma_s0_k <- qnorm(p00_k)
gamma_s1_k <- qnorm(p01_k)

# calculate first iteration of parameters

mu_s0_k <- (sum(p_s0_T))^(-1) * sum(y * p_s0_T)
mu_s1_k <- (sum(p_s1_T))^(-1) * sum(y * p_s1_T)
sigma2_s0_k <- sum( (y-mu_s0_k)^2 * p_s0_T ) / sum(p_s0_T)
sigma2_s1_k <- sum( (y-mu_s1_k)^2 * p_s1_T ) / sum(p_s1_T)

theta_k <- rbind(mu_s0_k,mu_s1_k,log(sigma2_s0_k),log(sigma2_s1_k),gamma_s0_k,gamma_s1_k)

# measure the distance between new estimates and initial values 

x <- theta_k-start_val
dist <- crossprod(x)

# use theta_k becomes theta_k_1 to compute next iteration

theta_k_1 <- theta_k

# count iterations 

count <- 1

# start_val given in the following order (mu_s0,mu_s1,sigma2_s0,sigma2_s1,gamma_s0,gamma_s1)

while(dist > tol) {

out <- hamilton.filter(theta_k_1,y,ss.prob = TRUE)

fprob_s1 <- out$f.prob_s1
fprob_s0 <- out$f.prob_s0
p_s0_t_1 <- out$p_s0_t_1
p_s1_t_1 <- out$p_s1_t_1
T.mat <- out$T.mat

s.out <- kim.smooth(T.mat,fp_s0 = out$f.prob_s0 , fp_s1 = out$f.prob_s1, p_s0_t_1 = out$p_s0_t_1, p_s1_t_1 = out$p_s1_t_1)

p_s0_T <- s.out$p_s0_T
p_s1_T <- s.out$p_s1_T
p_s0_s0_t_1 <- s.out$p_s0_s0_t_1
p_s0_s1_t_1 <- s.out$p_s0_s1_t_1
p_s1_s0_t_1 <- s.out$p_s1_s0_t_1
p_s1_s1_t_1 <- s.out$p_s1_s1_t_1

# iteration of the transition probabilities 

p00_k <- sum(p_s0_s0_t_1) / sum(p_s0_T[2:length(p_s0_T)])

p01_k <- sum(p_s0_s1_t_1) / sum(p_s1_T[2:length(p_s1_T)])

# recall we used a probit parameterization for the probabilities so we need to backout gamma_s0 and gamma_s1 using qnorm()

gamma_s0_k <- qnorm(p00_k)
gamma_s1_k <- qnorm(p01_k)

# calculate first iteration of parameters

mu_s0_k <- (sum(s.out$p_s0_T))^(-1) * sum(y * p_s0_T)
mu_s1_k <- (sum(s.out$p_s1_T))^(-1) * sum(y * p_s1_T)
sigma2_s0_k <- sum( (y-mu_s0_k)^2 * p_s0_T ) / sum(p_s0_T)
sigma2_s1_k <- sum( (y-mu_s1_k)^2 * p_s1_T ) / sum(p_s1_T)

theta_k <- rbind(mu_s0_k,mu_s1_k,log(sigma2_s0_k),log(sigma2_s1_k),gamma_s0_k,gamma_s1_k)

# measure the distance between new estimates and initial values 

x <- theta_k - theta_k_1
dist <- crossprod(x)

# use theta_k becomes theta_k_1 to compute next iteration

theta_k_1 <- theta_k

# count iterations

count <- count + 1

}

final.out <- hamilton.filter(theta_k,y,ss.prob = TRUE)
rownames(theta_k) <-c("mu_s0", "mu_s1", "log(sigma2_s0)", "log(sigma2_s1)", "gamma_s0", "gamma_s1") 

result <- list(theta_k = theta_k, distance = dist, tolerance = tol, no.iter = count, logLik = final.out$logLik, T.mat = final.out$T.mat)
return(result)

}

########################################################
########################################################

# # # # # trade signalling function # # # # #

trade.signal <- function(ratio,mle.pars,prob_s0,prob_s1,delta,rho) {
 
# ratio = Pa_t/Pb_t 

n <- length(ratio)
st <-rep(0,n)

high_state <- which.max(mle.pars[1:2])
low_state <- which.min(mle.pars[1:2])

if(high_state == 1) {
 
 high_sd <- exp(mle.pars[3])
 low_sd <- exp(mle.pars[4])
 prob_high <- prob_s0
 prob_low <- prob_s1
 }
 
 else {
  
  high_sd <- exp(mle.pars[4])
  low_sd <- exp(mle.pars[3])
  prob_high <- prob_s1
  prob_low <- prob_s0
  }


for(i in 1:n) {

if((ratio[i] < mle.pars[high_state] - delta * high_sd) && prob_high[i] > rho) {
 
 st[i] <- 1
 
 }

if(ratio[i] > mle.pars[high_state] + delta * high_sd) {
 
 st[i] <- -1
 
 }

if((ratio[i] > mle.pars[low_state] + delta * low_sd) && prob_low[i] > rho) {
 
 st[i] <- -1
 
 }
 
if(ratio[i] < mle.pars[low_state] - delta * low_sd) {
 
 st[i] <- 1
 
 }
 
st <- zoo(st, index(ratio))

}

return(st)

}
########################################################
########################################################

# # # # backtest function # # # #

back.test <- function(benchmark,numerator,denominator,signal) {
 
 require(quantmod)
 
 n <- length(signal)
 num_signal <-rep(0,n)
 denom_signal <- rep(0,n)
 
 for(i in 1:n) {
  
  if(signal[i] == 1) {
   
   num_signal[i] <- 1
   denom_signal[i] <- -1
   
   }
   
  if(signal[i] == -1) {
   
   num_signal[i] <- -1
   denom_signal[i] <- 1
   
   }
   
    }
    
  num_signal <- zoo(num_signal, index(signal))
  denom_signal <-zoo(denom_signal, index(signal))
  x <- zoo(0,index(signal[1])) 
  nsig <- rbind(x,lag(num_signal, k =-1))
  dsig <- rbind(x,lag(denom_signal, k =-1))
  
  benchmark.ret <- dailyReturn(benchmark, type = c("log"))
  numerator.ret <- dailyReturn(numerator, type = c("log"))
  denominator.ret <- dailyReturn(denominator, type = c("log"))
        
  benchmark.ret <- as.zoo(benchmark.ret)
  numerator.ret <- as.zoo(numerator.ret)
  denominator.ret <- as.zoo(denominator.ret)
 
  strat_return <- exp(cumsum(numerator.ret*nsig)+cumsum(denominator.ret*dsig))
  benchmark_return <- exp(cumsum(benchmark.ret))
  numerator_return <- exp(cumsum(numerator.ret))
  denominator_return <- exp(cumsum(denominator.ret))
  
  return(list(numerator_signal = nsig, denominator_signal = dsig,benchmark_return = benchmark_return,numerator_return = numerator_return, denominator_return = denominator_return, strat_return = strat_return))
  
  }
  
########################################################
########################################################
########################################################
########################################################
    
########################################################
########################################################
############ IMPLEMENTATION OF THE STRATEGY ############
########################################################
########################################################

library(quantmod)

# # # # # # # # # # get stock price data # # # # # # # # # #

getSymbols("USO", from = "2011-01-01", to = "2012-01-01")
getSymbols("TQQQ", from = "2011-01-01", to = "2012-01-01")

stock.11.1 <- Cl(USO)
stock.11.2 <- Cl(TQQQ)

# # calculate price ratios # # 

price.ratio.2011 <- stock.11.1/stock.11.2

# # # # # # # # # # parameter estimation # # # # # # # # # #

# pick start values for the 6 unknown parameters
# k is the number of times the EM algorithm is run
# different starting values for each run

k <- 5 
start_val <- matrix(rnorm(6*k), nrow = 6, ncol = k)

# matrix to store parameter estimates

par.cand <- matrix(NA,nrow = 6, ncol = k)

# row vector to store likelihood values

like.cand <- matrix(NA, nrow = 1, ncol = k)


for(i in 1:k) {



x <-try(EM(price.ratio.2011,tol = 1e-24,start_val[,i]))



if(class(x) == "try-error") next


par.cand[,i] <- x$theta_k
like.cand[,i] <-x$logLik

cat("Iteration", i, "of", k, "\n")

}

results <- rbind(like.cand,par.cand)

# # # select mle from candidates # # # 

like.cand <- results[1,]
index <- which.max(like.cand)
mle.pars <- results[-1,index]

# # # identify which regime is the high mean regime # # # 

which.max(mle.pars[1:2]) 

# # # # # # # # # # # # # # # # # # # # # # # # #  
# # # # # # # # # # # # # # # # # # # # # # # # # 

# # # get filtered probabilities # # #

output <- hamilton.filter(mle.pars,price.ratio.2010,ss.prob = TRUE)

fprob_s0.2011 <- output$f.prob_s0
fprob_s1.2011 <- output$f.prob_s1
p_s0_t_1.2011 <- output$p_s0_t_1
p_s1_t_1.2011 <- output$p_s1_t_1

# # #  get transition matrix # # # 

T.mat <- output$T.mat

# # # get smoothed probabilities # # #

smoothed.output <- kim.smooth(T.mat,fprob_s0.2011,fprob_s1.2011,p_s0_t_1.2011,p_s1_t_1.2011)

p_s0_T.2011 <- smoothed.output$p_s0_T
p_s1_T.2011 <- smoothed.output$p_s1_T


# # # # # # # # # # get data for this year # # # # # # # # # #

getSymbols("USO", from = "2012-01-03")
getSymbols("TQQQ", from = "2012-01-03")
getSymbols("SPY", from = "2012-01-03")

stock.2012.1 <- Cl(USO)
stock.2012.2 <- Cl(TQQQ)
stock.2012.3 <- Cl(SPY)

# calculate new.price.ratio

price.ratio.2012 <- stock.2012.1/stock.2012.2

# get filtered probabilities for new data

# # # get filtered probabilities for new data # # #

output2 <- hamilton.filter(mle.pars,price.ratio.2011,ss.prob = TRUE)

fprob_s0.2012 <- output2$f.prob_s0
fprob_s1.2012 <- output2$f.prob_s1
p_s0_t_1.2012 <- output2$p_s0_t_1
p_s1_t_1.2012 <- output2$p_s1_t_1

# # #  get transition matrix # # # 

T.mat.2012 <- output2$T.mat

# # # get smoothed probabilities # # #

smoothed.output2 <- kim.smooth(T.mat.2012,fprob_s0.2012,fprob_s1.2012,p_s0_t_1.2012,p_s1_t_1.2012)

p_s0_T.2012 <- smoothed.output2$p_s0_T
p_s1_T.2012 <- smoothed.output2$p_s1_T


# get signal

signal <- trade.signal(ratio = price.ratio.2012, mle.pars, prob_s0 = p_s0_T.2012,prob_s1 = p_s1_T.2012,delta = 1.645,rho = 0.7)

# backtest model 

bt.results <- back.test(stock.2012.3,numerator=stock.2012.1,denominator=stock.2012.2,signal)

# # # # # # # # plot results # # # # # # # #

# plot returns 

plot.zoo(merge(bt.results$strat_return, bt.results$benchmark_return, bt.results$numerator_return, bt.results$denominator_return),plot.type = c("single"), col = c("red", "blue", "green", "yellow"), ylab = "", xlab = "Date", main = "Continuously Compounded Returns (2012-01-04 to 2016-03-03)")

legend("topleft", legend = c("strategy","ftse","aal","xta"), col = c("red", "blue", "green", "yellow"), lty = 1, bty = "n")

# plot price ratio and trading signals

chartSeries(price.ratio.2011,name = "Price Ratio of STOCK1 - STOCK2", theme = chartTheme("black"))
plot(addTA(signal > 0, border = NA, col = "blue", on = -1))
plot(addTA(signal == 0, border = NA, col = "green", on = -1))
plot(addTA(signal < 0, border = NA, col = "red", on = -1))
addTA(zoo(p_s1_T.2012,index(price.ratio.2012)), legend = "Smoothed probability of high regime") 
#http://economics-notepad.blogspot.in/2011/12/markov-switching-pairs-trading-rule.html
#########END#######







***
http://economics-notepad.blogspot.in/2011/12/markov-switching-pairs-trading-rule.html
http://ses.library.usyd.edu.au/bitstream/2123/4072/1/Thesis_Schmidt.pdf
http://gekkoquant.com/2013/01/21/statistical-arbitrage-trading-a-cointegrated-pair/
https://tspace.library.utoronto.ca/bitstream/1807/65615/1/ZhengqinZeng_20146_MASc_thesis_pdf.pdf
http://myweb.fcu.edu.tw/~chenws/pairs.pdf
http://stat.wharton.upenn.edu/~steele/Courses/434/434Context/PairsTrading/PairsTradingQFin05.pdf
http://www.r-bloggers.com/the-fear-index-is-the-vix-efficient-to-be-warned-about-high-volatility-finance-systematic-processus/
http://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/
http://www.gauravbiware.com/pairs-trading-using-correlation-in-r/
http://repository.upenn.edu/cgi/viewcontent.cgi?article=1095&context=wharton_research_scholars
https://www.stat.berkeley.edu/~aldous/Research/Ugrad/Amy_Zhang.pdf
http://mobile.library2.smu.ca/bitstream/handle/01/26133/Bharadwaj_MandaRaghavaSantosh_MRP_2014.pdf?sequence=1&isAllowed=y
http://www.inside-r.org/packages/cran/PairTrading/docs/PairTrading
http://complete-markets.com/2011/03/fin-476-pairs-trade-r-code-trading-period/
http://gekkoquant.com/2012/10/21/statistical-arbitrage-correlation-vs-cointegration/
http://www.marketcalls.in/amibroker/pair-trading-amibroker-afl-code.html
http://files.meetup.com/1704326/TradingStrategies.pdf
https://cran.r-project.org/web/packages/egcm/egcm.pdf
http://gekkoquant.com/2012/10/
http://epchan.blogspot.in/2011/06/when-cointegration-of-pair-breaks-down.html

https://www.quantopian.com/posts/$gld-slash-$iau-pairs-trading-template
https://www.quantopian.com/posts/pairs-trading-algorithm
https://www.quantopian.com/posts/how-to-build-a-pairs-trading-strategy-on-quantopian
https://www.quantopian.com/posts/augmented-dickey-fuller-adf-test-spy-and-sh-dollar-volume
https://www.quantopian.com/posts/pairs-trading-algorithm
https://www.quantopian.com/posts/multiple-pairs-trading-algo
https://www.quantopian.com/posts/pairs-trading-algorithm-1
https://www.quantopian.com/posts/first-attempt-at-pairs-trading
https://www.quantopian.com/posts/questions-on-trading-cointegrated-pairs
https://www.quantopian.com/posts/identifying-cointegrated-pairs-in-a-sector
http://econ.schreiberlin.de/software/vecmclass.py
http://www.ccsenet.org/journal/index.php/ijef/article/view/33007
https://www.quantopian.com/posts/pairs-trading-with-error-correction-model
https://www.quantopian.com/posts/kalman-filter-multiple-pairs-trading
https://www.quantopian.com/posts/pair-trade-using-a-risk-factor-model
https://www.quantopian.com/posts/trading-strategy-worthy-of-q-fund
https://www.quantopian.com/posts/ernie-chan-uso-slash-gld-oil-slash-gold-pair-spread-trading-at-constant-leverage
https://www.quantopian.com/posts/pair-trade-with-cointegration-and-mean-reversion-tests
https://www.quantopian.com/posts/quantopian-lecture-series-this-time-youre-more-wrong?c=1
https://www.quantopian.com/posts/augmented-dickey-fuller-adf-test-spy-and-sh-dollar-volume
https://www.quantopian.com/posts/apt-and-pairs-trading-by-pca-between-spy-and-10-us-market-sectors-2
https://www.quantopian.com/posts/sensitivity-analysis-aka-parameter-optimization-of-pair-trade-input-parameters
https://www.quantopian.com/posts/cointegration-rsi-signals-long-only
https://www.quantopian.com/posts/machine-learning-turn-$10k-into-2-dollars-dot-25m-in-two-years-plus-22407-percent-returns-by-trading-brk-a-berkshire-hathaway-with-random-forest
https://www.quantopian.com/posts/trading-strategy-statistical-arbitrage-and-mean-reversion
https://www.quantopian.com/posts/brent-slash-wti-spread-fetcher-example
https://www.quantopian.com/posts/cvxvsxom
https://www.quantopian.com/posts/what-about-vxup-slash-vxdn
https://www.quantopian.com/posts/further-experiments-with-randomness
https://www.quantopian.com/posts/ernie-chans-ewa-slash-ewc-pair-trade-with-kalman-filter
https://www.quantopian.com/posts/october-prize-number-1-algo-disqualified-worthy-of-the-q-fund
https://www.quantopian.com/posts/very-simple-2-sharpe-algo-on-xle-components
https://www.quantopian.com/posts/improved-minimum-variance-portfolio

https://www.quantopian.com/posts/russell-2000-statistical-arbitrage-with-hedged-stock-pairs-mean-reversion-strategy

import datetime
import pytz
import numpy as np
import pandas as pd
import statsmodels.api as sm

WINDOW_LENGTH = 50 

def ols_transform(prices, sid1, sid2): # receives constantly updated dataframe
    """Computes regression coefficient (slope)
    via Ordinary Least Squares between two SIDs.
    """
    prices = prices.fillna(method='bfill')
    p0 = prices[sid1].values
    p1 = prices[sid2].values
        
    slope = sm.OLS(p0, p1).fit().params[0]
    return slope

def initialize(context):
    context.gld = sid(26807)
    context.iau = sid(26981)
    
    context.rebalance_date = None
    context.rebalance_trigger = 20

    # maximum total exposure (longs - shorts) in $US allowed
    context.max_notional = 1000000 #$30,000

    context.spreads = []
    
    set_commission(commission.PerTrade(cost=1.00))
    set_slippage(slippage.FixedSlippage(spread=0.00))

def handle_data(context, data):
    
    price_history = history(50, '1d', 'price')
    price_history = price_history.fillna(method='ffill')
    
    # Get the current exchange time, in local timezone 
    exchange_time = pd.Timestamp(get_datetime()).tz_convert('US/Eastern')   
    
    
    # Only run the OLS regression once per day at 10:30am
    if exchange_time.hour != 10 or exchange_time.minute != 30:
        return
    
    ######################################################
    # 1. Compute regression coefficient between GLD and IAU using the Ordinary Least Squares method
    # ref: http://en.wikipedia.org/wiki/Ordinary_least_squares
    params = ols_transform(price_history, context.gld, context.iau)
    
    context.slope = params

    ######################################################
    # 2. Compute zscore of spread (remove mean and divide by std), require at least 20 data points before 1st trade
    zscore = compute_zscore(context, data)
    record(zscore=zscore)
    
    if len(context.spreads)<20:
        log.info('fewer than 20 data points to z-score')
        return
    ######################################################
    # 3. Place orders (if its been the required # days since the position was initiated)
    if context.rebalance_date == None:
       place_orders(context, data, zscore, exchange_time)
        
    elif context.rebalance_date and exchange_time > context.rebalance_date + datetime.timedelta(days=context.rebalance_trigger):
        place_orders(context, data, zscore, exchange_time)

def compute_zscore(context, data):
    """1. Compute the spread given slope from the OLS regression
       2. zscore the spread.
    """
    spread = data[context.gld].price - (context.slope * data[context.iau].price)
    # Positive spread means that GLD is priced HIGHER than it should be relative to IAU
    # Negative spread means that GLD is priced LOWER than it should be relative to IAU
    
    context.spreads.append(spread)
          
    zscore = (spread - np.mean(context.spreads[-WINDOW_LENGTH:])) / np.std(context.spreads[-WINDOW_LENGTH:])
    return zscore

def place_orders(context, data, zscore, exchange_time):
    """Buy spread if zscore is > 2, sell if zscore < -2
    """
    # calculate the current notional value of each position
    notional1 = context.portfolio.positions[context.gld].amount * data[context.gld].price
    notional2 = context.portfolio.positions[context.iau].amount * data[context.iau].price
    #record(gld_pos_x10k=(0.0001)*notional1,iau_pos_x10k=(0.0001)*notional2)
    

    bet_size = 500000 #allocate $10,000 per side to the trade
    bet_shares_gld = bet_size / data[context.gld].price
    bet_shares_iau = bet_size / data[context.iau].price
    
    # if our notional invested is non-zero check whether the spread has narrowed to where we want to close positions:
    if abs(notional1) + abs(notional2) != 0:
        if zscore <= 0.5 and zscore >= -0.5:
            close_position(context, context.gld, data)
            close_position(context, context.iau, data)
            log.info('closing positions')
        else:
            return
        
    # if our notional invested is zero, check whether the spread has widened to where we want to open positions:
    elif abs(notional1) + abs(notional2) == 0:
        if zscore >= 2.0:
            # sell the spread, betting it will narrow since it is over 2 std deviations
            # away from the average
            order(context.gld, -1 * bet_shares_gld)
            order(context.iau, bet_shares_iau)
            log.info('z-score > 2, selling the spread')
            context.rebalance_date = exchange_time
    
        elif zscore <= -2.0:
            # buy the spread
            order(context.gld, bet_shares_gld)
            order(context.iau, -1 * bet_shares_iau)
            log.info('z-score < 2, buying the spread')
            context.rebalance_date = exchange_time


def close_position(context, stock, data):
    """
    decrease exposure to zero, regardless of position long/short.
    buy for a short position, sell for a long.
    """
    pos_amount = context.portfolio.positions[stock].amount
    order(stock, -1 * pos_amount)

******* ANOTHER

import numpy as np
import statsmodels.api as sm
import pandas as pd
from zipline.utils import tradingcalendar
import pytz


def initialize(context):
    # Quantopian backtester specific variables
    set_slippage(slippage.FixedSlippage(spread=0))
    set_commission(commission.PerTrade(cost=1))
    set_symbol_lookup_date('2014-01-01')
    context.Y = symbol('XLU')
    context.X = symbol('UWTI')
    # set_benchmark(context.y)
    
    
    # strategy specific variables
    context.lookback = 20 # used for regression
    context.z_window = 20 # used for zscore calculation, must be <= lookback
    
    context.useHRlag = True
    context.HRlag = 2
    
    context.spread = np.array([])
    context.hedgeRatioTS = np.array([])
    context.inLong = False
    context.inShort = False
    context.entryZ = 1.0
    context.exitZ = 0.0

    if not context.useHRlag:
        # a lag of 1 means no-lag, this is used for np.array[-1] indexing
        context.HRlag = 1
        
# Will be called on every trade event for the securities you specify. 
def handle_data(context, data):
   
    _Y_value = context.portfolio.positions[context.Y].amount * context.portfolio.positions[context.Y].last_sale_price
    _X_value = context.portfolio.positions[context.X].amount * context.portfolio.positions[context.X].last_sale_price
    _leverage = (abs(_Y_value) + abs(_X_value)) / context.portfolio.portfolio_value
    record(
            X_value = _X_value ,
            Y_value = _Y_value ,
            leverage = _leverage
    )
    
    if get_open_orders():
        return
    
    now = get_datetime()
    exchange_time = now.astimezone(pytz.timezone('US/Eastern'))
    
    if not (exchange_time.hour == 15 and exchange_time.minute == 30):
        return
    
    prices = history(35, '1d', 'price').iloc[-context.lookback::]

    Y = prices[context.Y]
    X = prices[context.X]

    try:
        hedge = hedge_ratio(Y, X, add_const=True)      
    except ValueError as e:
        log.debug(e)
        return
    
    context.hedgeRatioTS = np.append(context.hedgeRatioTS, hedge)
    # Calculate the current day's spread and add it to the running tally
    if context.hedgeRatioTS.size < context.HRlag:
        return
    # Grab the previous day's hedgeRatio
    hedge = context.hedgeRatioTS[-context.HRlag]  
    context.spread = np.append(context.spread, Y[-1] - hedge * X[-1])

    if context.spread.size > context.z_window:
        # Keep only the z-score lookback period
        spreads = context.spread[-context.z_window:]
        
        zscore = (spreads[-1] - spreads.mean()) / spreads.std()
          
        if context.inShort and zscore < 0.0:
            order_target(context.Y, 0)
            order_target(context.X, 0)
            context.inShort = False
            context.inLong = False
            record(X_pct=0, Y_pct=0)
            return
        
        if context.inLong and zscore > 0.0:
            order_target(context.Y, 0)
            order_target(context.X, 0)
            context.inShort = False
            context.inLong = False
            record(X_pct=0, Y_pct=0)
            return
            
        if zscore < -1.0 and (not context.inLong):
            # Only trade if NOT already in a trade
            y_target_shares = 1
            X_target_shares = -hedge
            context.inLong = True
            context.inShort = False
            
            (y_target_pct, x_target_pct) = computeHoldingsPct( y_target_shares,X_target_shares, Y[-1], X[-1] )
            order_target_percent(context.Y, y_target_pct)
            order_target_percent(context.X, x_target_pct)
            record(Y_pct=y_target_pct, X_pct=x_target_pct)
            return

        if zscore > 1.0 and (not context.inShort):
            # Only trade if NOT already in a trade
            y_target_shares = -1
            X_target_shares = hedge
            context.inShort = True
            context.inLong = False
           
            (y_target_pct, x_target_pct) = computeHoldingsPct( y_target_shares, X_target_shares, Y[-1], X[-1] )
            order_target_percent(context.Y, y_target_pct)
            order_target_percent(context.X, x_target_pct)
            record(Y_pct=y_target_pct, X_pct=x_target_pct)


def is_market_close(dt):
    ref = tradingcalendar.canonicalize_datetime(dt)
    return dt == tradingcalendar.open_and_closes.T[ref]['market_close']

def hedge_ratio(Y, X, add_const=True):
    if add_const:
        X = sm.add_constant(X)
        model = sm.OLS(Y, X).fit()
        return model.params[1]
    model = sm.OLS(Y, X).fit()
    return model.params.values
    
def computeHoldingsPct(yShares, xShares, yPrice, xPrice):
    yDol = yShares * yPrice
    xDol = xShares * xPrice
    notionalDol =  abs(yDol) + abs(xDol)
    y_target_pct = yDol / notionalDol
    x_target_pct = xDol / notionalDol
    return (y_target_pct, x_target_pct)

*****
getSymbols(c("SPY","USO","UCO","IWM","EEM","ECH","TLT","EWB","IWO","RSP","EWRS","TNA","UDOW","UPRO","VXX","XXV","KOLD","BOIL","ERX","ERY","NUGT","DUST"))

y1<-BOIL
y2<-KOLD
y<-merge(tail(y1,1000),tail(y2,1000))
rolling_correlation<-rollapply(y,width=50,FUN=function(y) cor(y[,1],y[,2]),by.column = FALSE, align = "right")
plot(rolling_correlation)

##Rolling Correlation
getSymbols(c("FAS","FAZ"))
y1<-Cl(FAS)
y2<-Cl(FAZ)
y<-merge(tail(y1,1000),tail(y2,1000))
rolling_correlation<-rollapply(y,width=50,FUN=function(y) cor(y[,1],y[,2]),by.column = FALSE, align = "right")
plot(tail(rolling_correlation,250))
##Rolling Regression
rr <- rollapply(y, width = 36,FUN = function(y) coef(lm(y[,1] ~ y[,2], data = as.data.frame(y))),by.column = FALSE, align = "right")
plot(tail(rr[,1],50))
plot(tail(rr[,2],50))


KOLD-BOIL
ERX-ERY
NUGT-DUST
SQQQ-TQQQ
FAS FAZ
OIL-USO
DIA-SPY
DIA-TLT
XLU-XLF
XLU-USO
XLU-UWTI
TQQQ-UWTI
EWW-EWP
EWC-EWA

** LOOK AHEAD BIAS
** SURVIVAL BIAS
**OVERFITTING






    DBO - DTO (PowerShares DB Oil Fund - Powershares DB Crude Oil Double Short ETN)
    DBO - GAZ (PowerShares DB Oil Fund - iPath Dow Jones-UBS Natural Gas Subindex Total Return ETN)
    DBO - USO (PowerShares DB Oil Fund - United States Oil Fund LP )
    AMJ - DBO (JPMorgan Alerian MLP Index ETN - PowerShares DB Oil Fund )
    DBO - UNG (PowerShares DB Oil Fund - United States Natural Gas Fund, LP)
    DBO - OIL (PowerShares DB Oil Fund - iPath S&P GSCI Crude Oil Total Return)
    DBO - SCO (PowerShares DB Oil Fund - ProShares UltraShort DJ-UBS Crude Oi ETF)
    DBO - UGA (PowerShares DB Oil Fund - United States Gasoline Fund, LP)
    DTO - UCO (Powershares DB Crude Oil Double Short ETN - ProShares Ultra DJ-UBS Crude Oil)
    GAZ - UCO (iPath Dow Jones-UBS Natural Gas Subindex Total Return ETN - ProShares Ultra DJ-UBS Crude Oil)
    UCO - USO (ProShares Ultra DJ-UBS Crude Oil - United States Oil Fund LP )
    UCO - UNG (ProShares Ultra DJ-UBS Crude Oil - United States Natural Gas Fund, LP)
    OIL - UCO (iPath S&P GSCI Crude Oil Total Return - ProShares Ultra DJ-UBS Crude Oil)
    SCO - UCO (ProShares UltraShort DJ-UBS Crude Oi ETF - ProShares Ultra DJ-UBS Crude Oil)
    DTO - GAZ (Powershares DB Crude Oil Double Short ETN - iPath Dow Jones-UBS Natural Gas Subindex Total Return ETN)
    DTO - USO (Powershares DB Crude Oil Double Short ETN - United States Oil Fund LP )
    AMJ - DTO (JPMorgan Alerian MLP Index ETN - Powershares DB Crude Oil Double Short ETN)
    DTO - UNG (Powershares DB Crude Oil Double Short ETN - United States Natural Gas Fund, LP)
    DTO - UGA (Powershares DB Crude Oil Double Short ETN - United States Gasoline Fund, LP)
    GAZ - USO (iPath Dow Jones-UBS Natural Gas Subindex Total Return ETN - United States Oil Fund LP )

http://www.pairslog.com/pairsList.php?startPos=20&
http://seekingalpha.com/article/3070136-shorting-leveraged-etf-pairs-easier-said-than-done
http://seekingalpha.com/article/1254391-3-leveraged-etf-pairs-to-short

getSymbols(c("SPY","USO","UCO","IWM","EEM","ECH","TLT","EWB","IWO","RSP","EWRS","TNA","UDOW","UPRO","VXX","XXV","KOLD","BOIL","ERX","ERY","NUGT","DUST"))

y1<-BOIL
y2<-KOLD
y<-merge(tail(y1,1000),tail(y2,1000))
rolling_correlation<-rollapply(y,width=50,FUN=function(y) cor(y[,1],y[,2]),by.column = FALSE, align = "right")
plot(rolling_correlation)

##Rolling Correlation
getSymbols(c("FAS","FAZ"))
y1<-Cl(FAS)
y2<-Cl(FAZ)
y<-merge(tail(y1,1000),tail(y2,1000))
rolling_correlation<-rollapply(y,width=50,FUN=function(y) cor(y[,1],y[,2]),by.column = FALSE, align = "right")
plot(tail(rolling_correlation,250))
##Rolling Regression
rr <- rollapply(y, width = 36,FUN = function(y) coef(lm(y[,1] ~ y[,2], data = as.data.frame(y))),by.column = FALSE, align = "right")
plot(tail(rr[,1],50))
plot(tail(rr[,2],50))

getSymbols(c("NUGT","DUST"))
ratio<-na.omit(Cl(NUGT)/Cl(DUST))
ra <- rollapply(ratio, width = 36,FUN = function(ratio) forecast.Arima(arima(ratio, order=c(2,1,2)), data = as.data.frame(ratio),by.column = FALSE, align = "right") 

arima <- arima(na.omit(ratio), order=c(2,0,2)) # fit an ARIMA(p,d,q) model
arimaforecasts <- forecast.Arima(arima, h=1)
arimaforecasts 
http://gekkoquant.com/2013/01/21/statistical-arbitrage-trading-a-cointegrated-pair/
http://www.rfortraders.com/lecture-4-regression-and-pairs-trading/
http://www.r-bloggers.com/simulating-backtests-of-stock-returns-using-monte-carlo-and-snowfall-in-parallel/
http://www.r-bloggers.com/pair-trading-strategy-how-to-use-pairtrading-package/
library("zoo", lib.loc="~/R/win-library/3.1")
coke_dates <- as.Date(coke[,1])
pep_dates  <- as.Date(pep[,1])
cc         <- zoo(coke[,5], coke_dates)
pp         <- zoo(pep[,5], pep_dates)
t.zoo      <- merge(cc, pp, all=FALSE)
t          <- as.data.frame(t.zoo)
r1         <- princomp( ~ log(coke$Close) + log(pep$Close))
slope1     <- r1$loadings[2,1]/r1$loadings[1,1]
prd        <- t$cc - slope1*t$pp
library(tseries)
ht         <- adf.test(prd, alternative="stationary", k=0)


http://repository.upenn.edu/cgi/viewcontent.cgi?article=1095&context=wharton_research_scholars
http://www.gauravbiware.com/pairs-trading-using-correlation-in-r/
https://www.stat.berkeley.edu/~aldous/Research/Ugrad/Amy_Zhang.pdf
http://mysimplequant.blogspot.in/2011/02/learning-spread-trades.html
https://www.quantopian.com/posts/writing-a-thesis-on-pair-trading-strategies-should-i-use-quantopian-for-research

http://stackoverflow.com/questions/30178611/pairs-trading-in-r-calculating-and-maximizing-profit
http://www.yats.com/doc/cointegration-en.html
https://sinamotamedi.wordpress.com/2009/04/16/pairs-trading-in-r/
http://swfa2015.uno.edu/B_Asset_Pricing_III/paper_196.pdf
https://www3.nd.edu/~pgao/papers/EngelbergGaoJag_31August2008.pdf
http://mobile.library2.smu.ca/bitstream/handle/01/26133/Bharadwaj_MandaRaghavaSantosh_MRP_2014.pdf?sequence=1&isAllowed=y
http://files.meetup.com/1704326/TradingStrategies.pdf

https://cran.r-project.org/web/views/TimeSeries.html
https://cran.r-project.org/web/views/Econometrics.html
https://cran.r-project.org/web/views/Finance.html
https://github.com/ljump12/Pairs-Trading/blob/master/pairs.R
http://r.789695.n4.nabble.com/How-to-test-pairs-trading-strategy-td3558776.html
http://remington-research.blogspot.in/2013/04/cointegration-pair-trading.html
http://coin.wne.uw.edu.pl/pwojcik/hfd_en.html
https://www.coursehero.com/file/6904571/Section-7-R-code/
http://waxworksmath.com/authors/n_z/vidyamurthy/writeup/weatherwax_vidyamurthy_notes.pdf
http://brage.bibsys.no/xmlui/bitstream/handle/11250/221265/masterthesis.pdf?sequence=1
http://ses.library.usyd.edu.au/bitstream/2123/4072/1/Thesis_Schmidt.pdf
http://www.aueb.gr/conferences/Crete2011/more%20recent/Schizas.pdf

************
Pairs trading is typically classified as an EMN investment strategy that is based on short term price reversal.  The strategy, characterised in industry as a form of statistical arbitrage, involves finding two stocks that follow a similar trading pattern.  When the stock prices deviate from a specified trading rule, positions are taken long in the lower priced security and short in the higher priced security, with the expectation that prices will converge.  The strategy derives returns from the well documented cross autocorrelation in stocks.  Recent microstructure research has documented the relationship between liquidity and cross-autocorrelation and Engelberg,Gao, and Jagannathan (2009)  show that when information is common, market frictions such as illiquidity cause a lead lag relationship in pairs of stocks,  leading to profitable pairs trading. 

Elliott, Van Der Hoek, and Malcolm (2005) use a Gaussian Markov chain model for the spread while Do, Faff, and Hamza (2006) make adaptations for spread measurement based on theoretical asset pricing methods and mean reversion. Vidyamurthy (2004) and Burgess (2005) utilize cointegration for pairs selection, while Papadakis and Wysocki (2007) expand on the methodology of Gatev et al. (2006) by examining the impact of accounting information events 
(i.e. earnings announcements and analyst forecasts) on the level of returns of the pairs trading strategy.

1. Evidence on the profitability of the strategy in the INDIA
2. Analysis of the strategy during the 2008 financial crisis highlighting that unlike other EMN strategies, pairs trading performed 
well in the crisis.
3. Comprehensive analysis of the risks in the strategy using a multi-factor model framework
4. Time series performance of the strategy with equity market liquidity, price impact and different estimates of bid ask spreads.
5. We provide an analysis of the performance of the strategy across different market and economic states
6. 


Elliott, Van Der Hoek, and Malcolm (2005) use a Gaussian Markov chain model for the spread while Do, Faff, and Hamza (2006) make adaptations for spread measurement based on theoretical asset pricing methods and mean reversion. Vidyamurthy (2004) and Burgess (2005) utilize cointegration for pairs selection, while Papadakis and Wysocki (2007) expand on the methodology of Gatev et al. (2006) by examining the impact of accounting information events 
(i.e. earnings announcements and analyst forecasts) on the level of returns of the pairs trading strategy.


To examine the risk exposure of the pairs trading portfolios, the returns are regressed against the common equity factors, market, size, value, momentum, and reversal.  Overall, only the market factor is significantly related to the top five and twenty pairs trading portfolios and the factor model has low explanatory power.  When the portfolio is separated into long and short portfolios the exposure to common risk factors greatly increases.  Exposure to the market and size factor is positive and significant for the long portfolios and negative and significant for the short portfolios, whereas when the portfolios are combined the systematic risk exposure is hedged.   


To ensure our results are directly comparable to the US, and to avoid potential data-mining bias, the methodology closely follows the work of Gatev et al. (2006).  The pairs trading portfolio is formed over two periods, a formation and trading period.  Pairs of stocks are matched during a twelve month formation period, based on a specified trading rule and traded during a six month period immediately following the formation period. Transaction costs are modelled using an estimate of the bid ask spread.  Later in the paper, to test the robustness of transaction cost estimates from Gatev et al. (2006), we provide additional transaction cost estimates based upon both the quoted spread and effective spread (Roll (1984)). 

Pairs are formed by matching each stock with a second     stock   that   has   the   minimum sum of squared deviations between the normalized price series over the twelve month formation period.  The top five and twenty pairs, ranked by minimum sum of squared deviation, are matched at the end of each formation period and are traded over the following six month trading period.

https://www.ucc.ie/en/media/research/centreforinvestmentresearch/wp/PT2.5.pdf
the SPDR S&P 500 ETF (SPY) is used as a proxy for the market and the iShares Barclays 1-3 Year Treasury Bond Fund (SHY) is used as a proxy for the risk-free rate. The statistical programing language, R is used for the implementation of the strategy and the R packages RTAQ and PerformanceAnalytics are used for handling raw tick-by-tick data and performance analysis of portfolios. We define a pair is compatible with a pairs trading strategy if their
CAPM β's are with 0.15 of each other and if the logs of their price processes are cointegrated.β 's based on the Capital Asset Pricing Model (CAPM) differ by at most 0.15 and testing if they are cointegrated. 

The CAPM predicts the return of an asset, i as E(ri)=(E(rm)−rf)βi+rf , where rf is the return on the riskfree asset, and rm is the return on the market. From this equation, βi can be interpreted as the correlation of a security's return to the market's return. Thus, if the ratio βi /βj for two separate securities, i and j is close to one, then we expect them to be affected by market movements in the same fashion, a condition that favors pairs-trading compatibility. 

We then test the prices of the two assets for cointegration, an feature of attractive pairs. Cointegration, as defined by Engle and Granger is a statistical property that tests if two processes tend 

We test for cointegration using the Engle-Granger two step cointegration test. To do this, we performed the following steps. These steps are adapted from the procedure outlined in Vidyamurthy (2004):
 Fit a best fit line using Least Squares Linear Regression to the equation log(P1)=log(P2)∗βcoint , where P1 and P2 refer to the respective prices of each stock in the pair and the fitted parameter, βcoint , will be referred to as the cointegration ratio. We constrain the intercept to 0 since if pair is cointegrated, then we expect 0 returns on one asset to predict 0 returns on the other. 
 Construct the spread between the two assets after stripping out the effects of cointegration where the spread at time t, S t=log(P1)−βcoint∗log(P2)
 Test spread of pair for stationarity using an Augmented-Dickey Fuller (ADF) Test,which tests the null hypothesis that a process has a unit root (is not stationary). If the pair is cointegrated, then the spread should be stationary.
***page onwards
http://repository.upenn.edu/cgi/viewcontent.cgi?article=1095&context=wharton_research_scholars  



http://www.aueb.gr/conferences/Crete2011/more%20recent/Schizas.pdf
http://www.r-bloggers.com/using-cart-for-stock-market-forecasting/
Keywords: statistical arbitrage, pairs trading, mean reversion, Markow regime switching,
JEL Classification: C53, G12

[proposed strategy exhibit excess returns of 16.38% per year, Sharpe Ratio of 1.34 and low correlation with the market.]
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2196391&rec=1&srcabs=1213802&alg=1&pos=1

Bock & Mestel use a two-state markov-switching mean and variance model to describe the price ratio of two stocks and offer an intuitive trading rule. 
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1213802

http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2152324&rec=1&srcabs=2196391&alg=1&pos=1
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2432061&rec=1&srcabs=1213802&alg=1&pos=2
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=952782&rec=1&srcabs=1213802&alg=1&pos=3
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2222196&rec=1&srcabs=1213802&alg=1&pos=4
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1666799&rec=1&srcabs=1213802&alg=1&pos=5
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2140091&rec=1&srcabs=1666799&alg=1&pos=1
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1585517&rec=1&srcabs=1666799&alg=1&pos=3
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2018681&rec=1&srcabs=1666799&alg=1&pos=9
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1361293&rec=1&srcabs=1213802&alg=1&pos=6
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2491201&rec=1&srcabs=2150217&alg=1&pos=3
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2196391&rec=1&srcabs=2150217&alg=1&pos=2
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2432061&rec=1&srcabs=2150217&alg=1&pos=7
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2545440&rec=1&srcabs=2150217&alg=1&pos=8
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2150217&rec=1&srcabs=1213802&alg=1&pos=10
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=962461&rec=1&srcabs=1213802&alg=1&pos=9
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1594066&rec=1&srcabs=1361293&alg=1&pos=1
http://papers.ssrn.com/sol3/results.cfm?RequestTimeout=50000000
