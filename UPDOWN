#***
library("quantmod")
library("rpart")
library("rpart.plot")
library("caTools")
library("caret")
library("kernlab")
library("nnet")
library("foreign")
library("ggplot2")
library("reshape2")
library("PerformanceAnalytics")
library("car")
library("FinTS")
library("fGarch")
library("robustbase")
library("MASS")
library("klaR")
library("lubridate")#Makes it easier to work with the dates
library("e1071")
library("parallel")
library("DMwR")
library("randomForest")
library(‘depmixS4’)
library("forecast") 
#***
getSymbols(c("^GSPC",'^NDX','^OEX','^VIX','UWTI','DWTI','SQQQ','TQQQ','XLE','XLF','XLV','XLY','XLI','TLT','SST','EEM','CAD=X'))
***
F1: PRICE-VOLUME convergence or divergence ??
#Price and Volume change
q3<-na.omit(ROC(Cl(NDX)))
q2<-na.omit(Vo(NDX))
q1<-na.omit(Cl(NDX))
q4<-na.omit(ROC(Vo(NDX)))
rolling_quantile_CL<-rollapply(q1,width=50,FUN=function(y) quantile(y,c(0.01,0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95,0.99)),by.column = FALSE, align = "right")
rolling_quantile_VO<-rollapply(q2,width=50,FUN=function(y) quantile(y,c(0.01,0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95,0.99)),by.column = FALSE, align = "right")
rolling_quantile_CLR<-rollapply(q3,width=50,FUN=function(y) quantile(y,c(0.01,0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95,0.99)),by.column = FALSE, align = "right")
rolling_quantile_VOLR<-rollapply(q4,width=50,FUN=function(y) quantile(y,c(0.01,0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95,0.99)),by.column = FALSE, align = "right")
PU<-ifelse(ROC(Cl(NDX))>=0.0258,5,ifelse(ROC(Cl(NDX))>=0.021 & ROC(Cl(NDX))<0.0258,4,ifelse(ROC(Cl(NDX))>=0.012 & ROC(Cl(NDX))<0.021,3,ifelse(ROC(Cl(NDX))>=0.007 & ROC(Cl(NDX))<0.012,2,ifelse(ROC(Cl(NDX))>=0.002 & ROC(Cl(NDX))<0.007,1,0)))))
PD<-ifelse(ROC(Cl(NDX))<=(-0.0335),-5,ifelse(ROC(Cl(NDX))<=(-0.027) & ROC(Cl(NDX))>(-0.0335),-4,ifelse(ROC(Cl(NDX))<=(-0.014) & ROC(Cl(NDX))>(-0.027),-3,ifelse(ROC(Cl(NDX))<=(-0.005) & ROC(Cl(NDX))>(-0.014),-2,ifelse(ROC(Cl(NDX))<=(-0.0012) & ROC(Cl(NDX))>(-0.005),-1,0)))))
PCUD<-ifelse(ROC(Cl(NDX))>0,PU,PD)
VU<-ifelse(ROC(Vo(NDX))>=0.021,5,ifelse(ROC(Cl(NDX))>=0.01 & ROC(Cl(NDX))<0.02,4,ifelse(ROC(Cl(NDX))>=0.004 & ROC(Cl(NDX))<0.0075,2,ifelse(ROC(Cl(NDX))>=0.002 & ROC(Cl(NDX))<0.004,1,0))))
VD<-ifelse(ROC(Cl(NDX))<=(-0.028),-5,ifelse(ROC(Cl(NDX))<=(-0.016) & ROC(Cl(NDX))>(-0.028),-4,ifelse(ROC(Cl(NDX))<=(-0.01) & ROC(Cl(NDX))>(-0.016),-3,ifelse(ROC(Cl(NDX))<=(-0.005) & ROC(Cl(NDX))>(-0.01),-2,ifelse(ROC(Cl(NDX))<=(-0.0015) & ROC(Cl(NDX))>(-0.05),1,0)))))
VCUD<-ifelse(ROC(Cl(NDX))>0,VU,VD)
##Price and Volume
PL<-ifelse(Cl(NDX)>=rolling_quantile_CL$X1. & Cl(NDX)<rolling_quantile_CL$X5.,1,ifelse(Cl(NDX)>rolling_quantile_CL$X5. & Cl(NDX)<rolling_quantile_CL$X15.,2,ifelse(Cl(NDX)>=rolling_quantile_CL$X15. & Cl(NDX)<rolling_quantile_CL$X25.,3,ifelse(Cl(NDX)>=rolling_quantile_CL$X25. & Cl(NDX)<rolling_quantile_CL$X35.,4,ifelse(Cl(NDX)>=rolling_quantile_CL$X35. & Cl(NDX)<rolling_quantile_CL$X45.,5,ifelse(Cl(NDX)>=rolling_quantile_CL$X45. & Cl(NDX)<rolling_quantile_CL$X55.,6,ifelse(Cl(NDX)>=rolling_quantile_CL$X55. & Cl(NDX)<rolling_quantile_CL$X65.,7,ifelse(Cl(NDX)>=rolling_quantile_CL$X65. & Cl(NDX)<rolling_quantile_CL$X75.,8,ifelse(Cl(NDX)>=rolling_quantile_CL$X75. & Cl(NDX)<rolling_quantile_CL$X85.,9,ifelse(Cl(NDX)>=rolling_quantile_CL$X85. & Cl(NDX)<rolling_quantile_CL$X95.,10,ifelse(Cl(NDX)>=rolling_quantile_CL$X95. & Cl(NDX)<rolling_quantile_CL$X99.,11,ifelse(Cl(NDX)>=rolling_quantile_CL$X99.,12,0))))))))))))
VL<-ifelse(Vo(NDX)>=rolling_quantile_VO$X1. & Vo(NDX)<rolling_quantile_VO$X5.,1,ifelse(Vo(NDX)>rolling_quantile_VO$X5. & Vo(NDX)<rolling_quantile_VO$X15.,2,ifelse(Vo(NDX)>=rolling_quantile_VO$X15. & Vo(NDX)<rolling_quantile_VO$X25.,3,ifelse(Vo(NDX)>=rolling_quantile_VO$X25. & Vo(NDX)<rolling_quantile_VO$X35.,4,ifelse(Vo(NDX)>=rolling_quantile_VO$X35. & Vo(NDX)<rolling_quantile_VO$X45.,5,ifelse(Vo(NDX)>=rolling_quantile_VO$X45. & Vo(NDX)<rolling_quantile_VO$X55.,6,ifelse(Vo(NDX)>=rolling_quantile_VO$X55. & Vo(NDX)<rolling_quantile_VO$X65.,7,ifelse(Vo(NDX)>=rolling_quantile_VO$X65. & Vo(NDX)<rolling_quantile_VO$X75.,8,ifelse(Vo(NDX)>=rolling_quantile_VO$X75. & Vo(NDX)<rolling_quantile_VO$X85.,9,ifelse(Vo(NDX)>=rolling_quantile_VO$X85. & Vo(NDX)<rolling_quantile_VO$X95.,10,ifelse(Vo(NDX)>=rolling_quantile_VO$X95. & Vo(NDX)<rolling_quantile_VO$X99.,11,ifelse(Vo(NDX)>=rolling_quantile_VO$X99.,12,0))))))))))))
  #Plot
par(mfrow=c(2,2))
plot(VCUD["2016-1"])
plot(PCUD["2016-1"])
plot(PL["2016-1"])
plot(VL["2016-1"])
#F2:Volatility
chaiVOL<-na.omit(chaikinVolatility(NDX[,2:3]))
atrVOL<-na.omit(ATR(NDX[,2:4])[,2])
vGK <- Cl(volatility(NDX[,1:4], calc="garman"))
stochOsc <- na.omit(stoch(NDX[,2:4]))
stochWPR<- na.omit(WPR(NDX[,2:4]))
q1=Cl(NDX)
rollSD<-rollapply(q1,width=50,FUN=function(y) stdev(y)/mean(y),by.column = FALSE, align = "right")
#plot
par(mfrow=c(2,3))
plot(chaiVOL["2016-01"])#cor(tail(Cl(NDX),10),tail(chaiVOL,10)) is -72%
plot(atrVOL["2016-01"])#cor(tail(Cl(NDX),10),tail(atrVOL,10)) is -71%
plot(vGK["2016-01"])#cor(tail(Cl(NDX),10),tail(vGK,10)) is 99%
plot(tail(stochOsc[,"fastK"], 100), type="l",main="Fast %K and Williams %R", ylab="",ylim=range(cbind(stochOsc, stochWPR), na.rm=TRUE) )
lines(tail(stochWPR, 100), col="blue")
lines(tail(1-stochWPR, 100), col="red", lty="dashed")
plot(rollSD["2016-01"])#cor(tail(Cl(NDX),10),tail(rollSD,10)) -63%
#INDEX RATIO PLOTS
plot(Cl(NDX)/Cl(GSPC))
plot(Cl(NDX)/Cl(OEX))
plot(Cl(GSPC)/Cl(OEX))
plot(Cl(TQQQ)/Cl(SQQQ))
plot(Cl(TLT)/Cl(SST))
plot(Cl(TLT)/Cl(EEM))
#INDEX
plot(Cl(DWTI))
plot(Cl(UWTI))
plot(Cl(TLT))
plot(Cl(`CAD=X`))
*** PRICE GRAPH
CHOC=Cl(NDX)-Op(NDX)
CHC=Cl(NDX)-lag(Cl(NDX))
d1<-diff(Op(NDX))
logd1<-diff(log(Cl(NDX)))
sd1<-diff(sqrt(Cl(NDX)))
doc1<-na.omit(diff(CHOC))
logdoc1<-na.omit(log(CHOC))
sdoc1<-na.omit(diff(sqrt(CHOC)))
par(mfrow=c(3,3))
plot(Cl(NDX)["2016"],type='l',xlab='Time',ylab='Close Prices',main='Daily Close Prices')# only close price
plot(na.omit(CHC)["2016"],type='l',xlab='Time',ylab='Close Price difference from yesterday',main='Daily Close Price difference from yesterday')# Change in a day
plot(d1["2016"],type='l',xlab='Time',ylab='Difference',main='First Degree Differencing on Open Data')
plot(logd1["2016"],type='l',xlab='Time',ylab='Difference',main='First Degree Differencing on Logged close Data')
plot(sd1["2016"],type='l',xlab='Time',ylab='Difference',main='First Degree Differencing on Square-root close Data')
plot(CHOC["2016"],type='l',xlab='Time',ylab='Close Price difference from opening',main='Daily Close Prices difference from opening')# Change in a day
plot(doc1["2016"],type='l',xlab='Time',ylab='Difference',main='First Degree Differencing on CL-OP Data')
plot(logdoc1["2016"],type='l',xlab='Time',ylab='Difference',main='First Degree Differencing on Logged CL-OP Data')
plot(sdoc1["2015/2016"],type='l',xlab='Time',ylab='Difference',main='First Degree Differencing on Square-root CL-OP Data')
#F3#Technical Indicators
RSI5<-RSI(Op(NDX), n= 5)#Calculate a 5-period relative strength index (RSI) off the open price strength index (RSI) off the open price
EMA15<-EMA(Op(NDX),n=15)#Calculate a 5-period exponential moving average (EMA)
EMAcross<- Op(NDX)-EMA15#Let’s explore the difference between the open price and our 5-period EMA
SMA15<-SMA(Op(NDX),n=15)
Trend<-Op(NDX)-SMA15#Our measure of trend: the difference between the open price and the 15-period simple moving average.
MACD<-MACD(Op(NDX),fast = 10, slow = 20, signal = 5)#Calculate a MACD with standard parameters
MACDsignal<-MACD[,2]#Grab just the signal line to use as our indicator.
SMI<-SMI(Op(NDX),n=9,slow=15,fast=2,signal=5) #Stochastic Oscillator with standard parameters
SMI<-SMI[,1]#Grab just the oscillator to use as our indicator
**self indicator1
par(mfrow=c(2,2))
atrVOL<-na.omit(ATR(NDX[,2:4])[,2])
atrMA<-SMA(atrVOL,n=5)
SMA15<-SMA(Cl(NDX),n=15)
SMA50<-SMA(Cl(NDX),n=50)
roc1<-ROC(Cl(NDX),n=1,type="discrete")
roc5<-ROC(Cl(NDX),n=5,type="discrete")
roc10<-ROC(Cl(NDX),n=10,type="discrete")
roc15<-ROC(Cl(NDX),n=15,type="discrete")
wave<-roc1*0.5+(roc5*0.3+roc10*0.3+roc15*15)*0.5
atr<-atrVOL-atrMA
S15<-Op(NDX)-SMA15
S50<-Op(NDX)-SMA50
  *plot
plot(tail(wave,50))
plot(tail(atr,50))
plot(tail(S50,50))
plot(tail(S15,50))
plot(tail(RSI5,50))
plot(tail(EMAcross,50))
plot(tail(Trend,50))
plot(tail(MACDsignal,50))
plot(tail(SMI,50))
****** Rolling Correlation
y1<-TD$USD_CAD
y2<-TD$TQQQ_SQQQ
y<-merge(y1,y2)
rolling_correlation<-rollapply(y,width=50,FUN=function(y) cor(y[,1],y[,2]),by.column = FALSE, align = "right")
plot(rolling_correlation)
****Rolling Regression
rr <- rollapply(y, width = 36,FUN = function(y) coef(lm(y ~ y[,1] + y[,2], data = as.data.frame(y))),by.column = FALSE, align = "right")
plot(rr)
# VOlume and price relation
par(mfrow=c(3,1))
PriceChange<- (log(Cl(NDX)) - log(Op(NDX)))
par(mfrow=c(3,1))
y1<-PriceChange
y2<-ROC(NDX[,4])
y<-merge(y1,y2)
rolling_correlation<-rollapply(y,width=50,FUN=function(y) cor(y[,1],y[,2]),by.column = FALSE, align = "right")
plot(rolling_correlation["2015/2016"])
rr <- rollapply(y, width = 36,FUN = function(y) coef(lm(y[,2] ~ y[,1], data = as.data.frame(y))),by.column = FALSE, align = "right")
plot(rr[,1])
plot(rr[,2])
****CYCLE
#??lubridate
DayofWeek<-wday(TD, label=TRUE)
TD1<-data.frame(DayofWeek,TD)
table(TD1$DayofWeek,TD$PL)
table(TD1$DayofWeek,TD$VL)
TDm<-month(TD, label = TRUE)
TD2<-data.frame(TDm,TD)
table(TD2$TDm,TD2$PL)
table(TD2$TDm,TD2$VL)
TDm15<-month(TD["2015"], label = TRUE)
TD15<-data.frame(TDm15,TD["2015"])
table(TD15$TDm15,TD15$PL)
table(TD15$TDm15,TD15$VL)
####HMM
LogReturns <- log(Cl(NDX)) - log(Op(NDX)) #calculate the logarithmic returns
dataHMM<-na.omit(data.frame(tail(LogReturns,1000),tail(***,1000)))
colnames(dataHMM)<-c("LogReturns","***") #name our columns 
set.seed(1)
HMM<-depmix(list(LogReturns~1,***~1),data=dataHMM,nstates=4,family=list(gaussian(),gaussian())) 
HMMfit<-fit(HMM, verbose = FALSE)
print(HMMfit)
summary(HMMfit) 
HMMpost<-posterior(HMMfit)
head(HMMpost)
par(mfrow=c(3,1))
plot(HMMpost$state)
plot(tail(***,50))
plot(tail(Cl(NDX),50))

##vGK#good to check
##TQQQ/SQQQ ratio   #Not significant 
##NDX/GSPC ratio   #Not significant
##TltEem ratio  #Not significant
##wave==good to check
##atr== good to check
##EMAcross== good to check
##SMI== not significant
##trend==not significant
##RSI3==good to check
##stochOsc==good to check

##F4#REGRESSION PREDICTION
logNDX<- log(Cl(NDX))-log(Op(NDX))
acf(na.omit(logNDX), lag.max=20) #16
pacf(na.omit(logNDX), lag.max=20)#16 
##ARIMA
NDXarima <- arima(logNDX, order=c(2,0,2)) # fit an ARIMA(p,d,q) model
NDXarimaforecasts <- forecast.Arima(NDXarima, h=1)
NDXarimaforecasts 

acf(NDXarimaforecasts $residuals, lag.max=20)
Box.test(NDXarimaforecasts $residuals, lag=20, type="Ljung-Box")
##GARCH
garchfit<-garchFit(~ garch(2,2), data =NDXdiff, trace = FALSE)
predict(garchfit, n.ahead = 1, plot=TRUE, crit_val=2)
##SVM
##source("e1071.R")#https://gist.github.com/ivannp/4180399  #copy and let run otherwise
tt<-get( getSymbols( "^NDX") )
rets<-na.trim((log(Cl(tt))-log(Op(tt))), type="discrete" )
# only the first two features so that we may see some results in reasonable time
dataSVM<-svmFeatures( tt )[,c(1,2)]
#data <- na.exclude(merge(rets,dataSVM))
rets<- rets[index(dataSVM)]
data<-dataSVM[index(rets)]
stopifnot( NROW( rets ) == NROW( data ) )
forecastSVM<-svmComputeForecasts(
    data=data,
    history=100,
    response=rets,
    cores=8,#for wondow =1
    trace=T,
    modelPeriod="days",
    startDate="2014-12-15",
    endDate="2016-01-07",
    featureSelection="all" ) ##for UBUNTU cores=8
    ts(last(forecastSVM)$Forecasts)[1]*ts(last(Cl(GSPC)))[1]## predicted value by SVM regression
####F5###
TD<-ROC(Cl(NDX))
TD$CH<-ifelse((log(Cl(NDX))-log(Op(NDX)))>0,1,0)# NDX
TD$NDX.Close<-NULL
TD$NDX<-(log(Cl(NDX))-log(Op(NDX)))
TD$GSPC<-(log(Cl(GSPC))-log(Op(GSPC)))
TD$OEX<-(log(Cl(OEX))-log(Op(OEX)))
TD$NDX_OEX<-Cl(NDX)/Cl(OEX)
TD$NDX_GSPC<-Cl(NDX)/Cl(GSPC)
TD$OEX_GSPC<-Cl(OEX)/Cl(GSPC)
TD$TQQQ_SQQQ<-Cl(TQQQ)/Cl(SQQQ)
TD$USD_CAD<-(log(Cl(`CAD=X`))-log(Op(`CAD=X`)))
TD$TLT_EEM<-Cl(TLT)/Cl(EEM)
TD$XLE<-(log(Cl(XLE))-log(Op(XLE)))
TD$XLF<-(log(Cl(XLF))-log(Op(XLF)))
TD$XLV<-(log(Cl(XLV))-log(Op(XLV)))
TD$XLY<-(log(Cl(XLY))-log(Op(XLY)))
TD$XLI<-(log(Cl(XLI))-log(Op(XLI)))
TD$DWTI<-(log(Cl(DWTI))-log(Op(DWTI)))
TD$USDCAD<-(log(Cl(`CAD=X`))-log(Op(`CAD=X`)))
TD$PCUD<-PCUD
TD$VCUD<-VCUD
TD$PL<-PL
TD$VL<-VL
TD$vGK<-vGK
TD$rollSD<-rollSD
TD$stochOsc<-stochOsc
TD$stochWPR<-stochWPR 
TD$RSI5<-RSI5
TD$EMAcross<-EMAcross
TD$Trend<-Trend
TD$MACDsignal<-MACDsignal
TD$SMI<-SMI
TD$wave<-wave
TD$atr<-atr
TD$P<-Cl(NDX)
*Train and Test set
set.seed(1000)
split<-sample.split(TD$CH,SplitRatio = 0.75)
train<-subset(TD,split=TRUE)
test<-subset(TD,split=FALSE)    
****correlation search
M<-cor(tail(TD,5))
diag(M)<-0    
M<-na.omit(M)
*TREE
DecisionTree<-rpart(CH~PL+VL+PCUD+VCUD+Trend+EMAcross+RSI5+MACDsignal+SMI+wave+atr,data=TD, cp=.003)
prp(DecisionTree, type=2)
tail(TD)
DecisionTree<-rpart(CH~PL+Trend+EMAcross+RSI5+MACDsignal+SMI+wave+atr,data=TD, cp=.008)
prp(DecisionTree, type=2)
##*SVM classification for indicator selection

*SVM
    *Indictor prediction
PriceChange<- Cl(GSPC) - Op(GSPC)
Class<-ifelse(PriceChange>0,"UP","DOWN")
RSI<-round(RSI(Op(GSPC), n= 13))# indicator to add here
EMA5<-EMA(Op(GSPC),n=5)
#Calculate a 5-period exponential moving average (EMA)
EMAcross<- Op(GSPC)-EMA5
SMA50<-SMA(Op(GSPC),n=50)
Trend<-Op(GSPC)-SMA50#Our measure of trend: the difference between the open price and the 50-period simple moving average.
MACD<-MACD(Op(GSPC),fast = 12, slow = 26, signal = 9)
#Calculate a MACD with standard parameters
MACDsignal<-MACD[,2]
#Grab just the signal line to use as our indicator.
SMI<-SMI(Op(GSPC),n=13,slow=25,fast=2,signal=9) 
#Stochastic Oscillator with standard parameters
SMI<-SMI[,1]
U<-ifelse(ROC(Cl(GSPC))>=0.01,4,ifelse(ROC(Cl(GSPC))>=0.0075 & ROC(Cl(GSPC))<0.01,3,ifelse(ROC(Cl(GSPC))>=0.004 & ROC(Cl(GSPC))<0.0075,2,ifelse(ROC(Cl(GSPC))>=0.002 & ROC(Cl(GSPC))<0.004,1,0))))
D<-ifelse(ROC(Cl(GSPC))<=(-0.01),-4,ifelse(ROC(Cl(GSPC))<=(-0.0075) & ROC(Cl(GSPC))>(-0.01),-3,ifelse(ROC(Cl(GSPC))<=(-0.004) & ROC(Cl(GSPC))>(-0.0075),-2,ifelse(ROC(Cl(GSPC))<=(-0.002) & ROC(Cl(GSPC))>(-0.004),-1,0))))
UD<-ifelse(ROC(Cl(GSPC))>0,U,D)
L<-ifelse(Cl(NDX)>=rolling_quantile$X5. & Cl(NDX)<rolling_quantile$X10.,1,ifelse(Cl(NDX)>=rolling_quantile$X10. & Cl(NDX)<rolling_quantile$X20.,2,ifelse(Cl(NDX)>=rolling_quantile$X20. & Cl(NDX)<rolling_quantile$X30.,3,ifelse(Cl(NDX)>=rolling_quantile$X30. & Cl(NDX)<rolling_quantile$X40.,4,ifelse(Cl(NDX)>=rolling_quantile$X40. & Cl(NDX)<rolling_quantile$X50.,5,ifelse(Cl(NDX)>=rolling_quantile$X50. & Cl(NDX)<rolling_quantile$X60.,6,ifelse(Cl(NDX)>=rolling_quantile$X60. & Cl(NDX)<rolling_quantile$X70.,7,ifelse(Cl(NDX)>=rolling_quantile$X70. & Cl(NDX)<rolling_quantile$X80.,8,ifelse(Cl(NDX)>=rolling_quantile$X80. & Cl(NDX)<rolling_quantile$X90.,9,ifelse(Cl(NDX)>=rolling_quantile$X90. & Cl(NDX)<rolling_quantile$X95.,10,ifelse(Cl(NDX)>=rolling_quantile$X95.,11,0)))))))))))
**
dataSVM<-data.frame(Class,RSI,EMAcross)
colnames(dataSVM)<-c("Class","RSI","EMAcross")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~RSI+EMAcross,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=EMAcross,y=RSI))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM RSI and EMA Predictions",x="EMAcross",y="RSI",color="Training Predictions")

**
dataSVM<-data.frame(Class,Trend,EMAcross)
colnames(dataSVM)<-c("Class","Trend","EMAcross")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~Trend+EMAcross,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=EMAcross,y=Trend))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="EMAcross",y="Trend",color="Training Predictions")
**
dataSVM<-data.frame(Class,Trend,EMAcross)
colnames(dataSVM)<-c("Class","Trend","EMAcross")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~Trend+EMAcross,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=EMAcross,y=Trend))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="EMAcross",y="Trend",color="Training Predictions")
***
dataSVM<-data.frame(Class,MACDsignal,EMAcross)
colnames(dataSVM)<-c("Class","MACDsignal","EMAcross")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~MACDsignal+EMAcross,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=EMAcross,y=MACDsignal))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="EMAcross",y="MACDsignal",color="Training Predictions")
***
dataSVM<-data.frame(Class,SMI,EMAcross)
colnames(dataSVM)<-c("Class","SMI","EMAcross")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~SMI+EMAcross,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=EMAcross,y=SMI))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="EMAcross",y="SMI",color="Training Predictions")
***
dataSVM<-data.frame(Class,UD,EMAcross)
colnames(dataSVM)<-c("Class","UD","EMAcross")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~UD+EMAcross,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=EMAcross,y=UD))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="EMAcross",y="UD",color="Training Predictions")
****
dataSVM<-data.frame(Class,UD,SMI)
colnames(dataSVM)<-c("Class","UD","SMI")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~UD+SMI,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=SMI,y=UD))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="SMI",y="UD",color="Training Predictions")
****
dataSVM<-data.frame(Class,UD,MACDsignal)
colnames(dataSVM)<-c("Class","UD","MACDsignal")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~UD+MACDsignal,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=MACDsignal,y=UD))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="MACDsignal",y="UD",color="Training Predictions")
***
dataSVM<-data.frame(Class,UD,Trend)
colnames(dataSVM)<-c("Class","UD","Trend")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~UD+Trend,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=Trend,y=UD))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="Trend",y="UD",color="Training Predictions")
****
dataSVM<-data.frame(Class,UD,RSI)
colnames(dataSVM)<-c("Class","UD","RSI")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~UD+RSI,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=RSI,y=UD))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="RSI",y="UD",color="Training Predictions")
***
dataSVM<-data.frame(Class,L,SMI)
colnames(dataSVM)<-c("Class","L","SMI")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~L+SMI,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the #number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=SMI,y=L))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="SMI",y="L",color="Training Predictions")
***
dataSVM<-data.frame(Class,L,MACDsignal)
colnames(dataSVM)<-c("Class","L","MACDsignal")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~L+MACDsignal,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the #number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=MACDsignal,y=L))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="MACDsignal",y="L",color="Training Predictions")
***
dataSVM<-data.frame(Class,L,Trend)
colnames(dataSVM)<-c("Class","L","Trend")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~L+Trend,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the #number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=Trend,y=L))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="Trend",y="L",color="Training Predictions")
***
dataSVM<-data.frame(Class,L,EMAcross)
colnames(dataSVM)<-c("Class","L","EMAcross")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~L+EMAcross,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the #number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=EMAcross,y=L))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="EMAcross",y="L",color="Training Predictions")
***
dataSVM<-data.frame(Class,L,EMAcross)
colnames(dataSVM)<-c("Class","L","EMAcross")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~L+EMAcross,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the #number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=EMAcross,y=L))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="EMAcross",y="L",color="Training Predictions")
***
dataSVM<-data.frame(Class,L,RSI)
colnames(dataSVM)<-c("Class","L","RSI")
dataSVM<-na.omit(dataSVM)
set.seed(886)
split<-sample.split(dataSVM$Class, SplitRatio = 0.75)
trainSVM<-subset(dataSVM, split == TRUE)
testSVM<-subset(dataSVM, split == FALSE)
SVM<-svm(Class~L+RSI,data=trainSVM, kernel="radial",cost=1,gamma=1/2)
#Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the #number of inputs we are using
TrainingPredictions<-predict(SVM,testSVM,type="class")
#Run the algorithm once more over the training set to visualize the patterns it found
TestData<-data.frame(testSVM,TrainingPredictions)#Create a data set with the predictions
ggplot(testSVM,aes(x=RSI,y=L))+stat_density2d(geom="contour",aes(color=TrainingPredictions))+labs(title="SVM Predictions",x="RSI",y="L",color="Training Predictions")

